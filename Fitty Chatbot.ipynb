{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d22ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Kousik\n",
      "[nltk_data]     C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Kousik\n",
      "[nltk_data]     C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e19548",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\Kousik C\\Downloads\\megaGymDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f401b1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Desc</th>\n",
       "      <th>Type</th>\n",
       "      <th>BodyPart</th>\n",
       "      <th>Equipment</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Partner plank band row</td>\n",
       "      <td>The partner plank band row is an abdominal exe...</td>\n",
       "      <td>Strength</td>\n",
       "      <td>Abdominals</td>\n",
       "      <td>Bands</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Banded crunch isometric hold</td>\n",
       "      <td>The banded crunch isometric hold is an exercis...</td>\n",
       "      <td>Strength</td>\n",
       "      <td>Abdominals</td>\n",
       "      <td>Bands</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>FYR Banded Plank Jack</td>\n",
       "      <td>The banded plank jack is a variation on the pl...</td>\n",
       "      <td>Strength</td>\n",
       "      <td>Abdominals</td>\n",
       "      <td>Bands</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Banded crunch</td>\n",
       "      <td>The banded crunch is an exercise targeting the...</td>\n",
       "      <td>Strength</td>\n",
       "      <td>Abdominals</td>\n",
       "      <td>Bands</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Crunch</td>\n",
       "      <td>The crunch is a popular core exercise targetin...</td>\n",
       "      <td>Strength</td>\n",
       "      <td>Abdominals</td>\n",
       "      <td>Bands</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                         Title  \\\n",
       "0      0        Partner plank band row   \n",
       "1      1  Banded crunch isometric hold   \n",
       "2      2         FYR Banded Plank Jack   \n",
       "3      3                 Banded crunch   \n",
       "4      4                        Crunch   \n",
       "\n",
       "                                                Desc      Type    BodyPart  \\\n",
       "0  The partner plank band row is an abdominal exe...  Strength  Abdominals   \n",
       "1  The banded crunch isometric hold is an exercis...  Strength  Abdominals   \n",
       "2  The banded plank jack is a variation on the pl...  Strength  Abdominals   \n",
       "3  The banded crunch is an exercise targeting the...  Strength  Abdominals   \n",
       "4  The crunch is a popular core exercise targetin...  Strength  Abdominals   \n",
       "\n",
       "  Equipment         Level  \n",
       "0     Bands  Intermediate  \n",
       "1     Bands  Intermediate  \n",
       "2     Bands  Intermediate  \n",
       "3     Bands  Intermediate  \n",
       "4     Bands  Intermediate  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd8b6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2872, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaed318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level\n",
       "Intermediate    2416\n",
       "Beginner         443\n",
       "Expert            13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fbae7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Equipment\n",
       "Body Only        1046\n",
       "Dumbbell          516\n",
       "Barbell           282\n",
       "Other             249\n",
       "Cable             226\n",
       "Machine           173\n",
       "Kettlebells       149\n",
       "Bands              97\n",
       "Medicine Ball      38\n",
       "Exercise Ball      35\n",
       "E-Z Curl Bar       22\n",
       "Foam Roll           9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Equipment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f0885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2872 entries, 0 to 2871\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Index      2872 non-null   int64 \n",
      " 1   Title      2872 non-null   object\n",
      " 2   Desc       1353 non-null   object\n",
      " 3   Type       2872 non-null   object\n",
      " 4   BodyPart   2872 non-null   object\n",
      " 5   Equipment  2842 non-null   object\n",
      " 6   Level      2872 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 157.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f9fe254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BodyPart\n",
       "Abdominals     662\n",
       "Quadriceps     646\n",
       "Shoulders      340\n",
       "Chest          262\n",
       "Biceps         168\n",
       "Triceps        151\n",
       "Lats           124\n",
       "Hamstrings     121\n",
       "Middle Back    118\n",
       "Lower Back      97\n",
       "Glutes          81\n",
       "Calves          47\n",
       "Forearms        31\n",
       "Traps           24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.BodyPart.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f833531",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83e3da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level\n",
       "Intermediate    2416\n",
       "Beginner         443\n",
       "Expert            13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd85c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tokenize(text):\n",
    "    if isinstance(text, float) and np.isnan(text):\n",
    "        return []\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbd7bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kousik C\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mtokenize, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDesc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBodyPart\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m decoder(doc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(data['Desc'])\n",
    "y = data['BodyPart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70820540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def welcome_message():\n",
    "    return \"Welcome to Fitty! Your personal gym assistant. How can I help you today?\"\n",
    "\n",
    "# Chatbot function\n",
    "def fitty_chatbot(query):\n",
    "    query_vectorized = tfidf_vectorizer.transform([query])\n",
    "    prediction = rf_classifier.predict(query_vectorized)\n",
    "    return prediction[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_exercises(body_part,difficulty_level):\n",
    "    body_parts = data['BodyPart'].unique()\n",
    "    random_exercises = {}\n",
    "    body_part_exercises = data[(data['BodyPart'] == body_part) & (data['Level'] == difficulty_level)]['Title']\n",
    "    random_exercise = body_part_exercises.sample(n=1).values[0]\n",
    "    random_exercises[body_part] = random_exercise\n",
    "    return random_exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a363013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_exercises(difficulty_level):\n",
    "    body_parts = data['BodyPart'].unique()\n",
    "    random_exercises = {}\n",
    "    for body_part in body_parts:\n",
    "        body_part_exercises = data[(data['BodyPart'] == body_part) & \n",
    "                                      (data['Level'] == difficulty_level)]['Title']\n",
    "        random_exercise = body_part_exercises.sample(n=1).values[0]\n",
    "        random_exercises[body_part] = random_exercise\n",
    "    return random_exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(user_input):\n",
    "    if 'plan' in user_input.lower():\n",
    "        level = input(\"Fitty: What is your fitness level (Beginner/Intermediate/Expert)? \")\n",
    "        if level.lower() in ['beginner', 'intermediate', 'expert']:\n",
    "            plan = get_random_exercises(level.capitalize())\n",
    "            return plan\n",
    "        else:\n",
    "            return \"Fitty: Please provide a valid fitness level.\"\n",
    "    elif any(body_part.lower() in user_input.lower() for body_part in data['BodyPart']):\n",
    "        matching_body_parts = [body_part for body_part in data['BodyPart'].unique() if body_part.lower() in user_input.lower()]\n",
    "        if matching_body_parts:\n",
    "            body_part = matching_body_parts[0]\n",
    "            level = input(f\"Fitty: What is your fitness level for {body_part}? (Beginner/Intermediate/Expert) \")\n",
    "            if level.lower() in ['beginner', 'intermediate', 'expert']:\n",
    "                return f\"Fitty:{part_exercises(body_part,level)}\"\n",
    "            else:\n",
    "                return \"Fitty: Please provide a valid fitness level.\"\n",
    "        else:\n",
    "            return \"Fitty: I'm sorry, I couldn't find any relevant body part in your query.\"\n",
    "    elif any(exercise.lower() in user_input.lower() for exercise in data['Title']):\n",
    "        matching_exercises = [exercise for exercise in data['Title'] if exercise.lower() in user_input.lower()]\n",
    "        if matching_exercises:\n",
    "            exercise = matching_exercises[0]\n",
    "            exercise_info = data[data['Title'] == exercise].iloc[0]\n",
    "            description = exercise_info['Desc']  # This line retrieves the description\n",
    "            equipment = exercise_info['Equipment']\n",
    "            return f\"Fitty: Exercise: {exercise}\\nDescription:{description}\\nEquipment: {equipment}\"\n",
    "        else:\n",
    "            return \"Fitty: I'm sorry, I couldn't find any exercise matching your query.\"\n",
    "    elif any(word in user_input.lower() for word in ['hello', 'hi', 'hey']):\n",
    "        return \"Fitty: Hello! I'm your personal gym assistant. How can I help you today?\"\n",
    "    elif 'bye' in user_input.lower():\n",
    "        return \"Fitty: Thank you! Goodbye.\"\n",
    "    else:\n",
    "        return f\"Fitty: I'm sorry, I didn't understand that. How can I assist you?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(welcome_message())\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        response = process_input(user_input)\n",
    "        print(response)\n",
    "        if 'bye' in user_input.lower():\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3dd7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4819f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
